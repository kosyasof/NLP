{"cells":[{"cell_type":"markdown","id":"3a564d5f","metadata":{"id":"3a564d5f"},"source":["#  Машинный перевод с использованием рекуррентных нейронных сетей\n","\n","__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n","\n","Материалы:\n","* Deep Learning with PyTorch (2020) Авторы: Eli Stevens, Luca Antiga, Thomas Viehmann\n","* https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-seq2seq-model\n","* https://www.adeveloperdiary.com/data-science/deep-learning/nlp/machine-translation-recurrent-neural-network-pytorch/"]},{"cell_type":"markdown","id":"c9ecd663","metadata":{"id":"c9ecd663"},"source":["## Задачи для совместного разбора"]},{"cell_type":"markdown","id":"05433855","metadata":{"id":"05433855"},"source":["1\\. Рассмотрите пример архитектуры Encoder-Decoder с использованием RNN. Обсудите концепцию teacher forcing."]},{"cell_type":"code","source":["import torch as th\n","import torch.nn as nn"],"metadata":{"id":"Nk_rIiMUqmbg","executionInfo":{"status":"ok","timestamp":1700596607367,"user_tz":-180,"elapsed":905,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"Nk_rIiMUqmbg","execution_count":1,"outputs":[]},{"cell_type":"code","source":["n_ru_tokens = 1000\n","batch_size = 16\n","ru_seq_len = 15\n","n_en_tokens = 500\n","en_seq_len = 10\n","\n","ru = th.randint(0, n_ru_tokens, size=(batch_size, ru_seq_len))\n","en = th.randint(0, n_en_tokens, size=(batch_size, en_seq_len))"],"metadata":{"id":"mtSMIzu3qmeM","executionInfo":{"status":"ok","timestamp":1700573715494,"user_tz":-180,"elapsed":244,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"mtSMIzu3qmeM","execution_count":32,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","  def __init__(self, embedding_dim, hidden_size):\n","      super().__init__()\n","      self.emb = nn.Embedding(\n","          num_embeddings=n_ru_tokens,\n","          embedding_dim=embedding_dim,\n","          padding_idx=0\n","      )\n","      self.dropout = nn.Dropout(p=0.5)\n","      self.rnn = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n","\n","  def forward(self, X):\n","    out = self.emb(X) # batch x seq x emb_size\n","    out = self.dropout(out)\n","    _, h = self.rnn(out) # out: batch x seq x hidden_size\n","    return h # 1 x batch x hidden_size"],"metadata":{"id":"yxJn5OjEqmhS"},"id":"yxJn5OjEqmhS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_dim = 100\n","encoder_hidden_size = 300\n","\n","encoder = Encoder(embedding_dim, encoder_hidden_size)\n","\n","encoder_output = encoder(ru)"],"metadata":{"id":"AJj4wUCjuSnq"},"id":"AJj4wUCjuSnq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_output.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JS2lE1f1oYuy","executionInfo":{"status":"ok","timestamp":1700498903837,"user_tz":-180,"elapsed":5,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"1be07fdc-b166-4f15-8bdb-ccc3635bcbb4"},"id":"JS2lE1f1oYuy","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 16, 300])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","  def __init__(self, embedding_dim, decoder_hidden_size):\n","    super().__init__()\n","    self.emb = nn.Embedding(\n","          num_embeddings=n_en_tokens,\n","          embedding_dim=embedding_dim,\n","          padding_idx=0\n","    )\n","    self.rnn = nn.GRUCell(embedding_dim, decoder_hidden_size)\n","    self.fc = nn.Linear(decoder_hidden_size, n_en_tokens)\n","\n","  def forward(self, encoder_output, labels):\n","    # labels: batch x seq_len - считаем, что в 0 столбце SOS\n","    # encoder_output: 1 x batch x encoder_hidden_size\n","    seq_len = labels.size(1)\n","    input_tokens = labels[:, 0]\n","    decoder_hidden = encoder_output[0]\n","    for _ in range(1, seq_len):\n","      out = self.emb(input_tokens).relu() # batch x emb_size\n","      decoder_hidden = self.rnn(out, decoder_hidden) # batch x dec_hidden\n","      out = self.fc(decoder_hidden) # batch x n_en_tokens\n","\n","      # teacher forcing\n","      input_tokens = out.argmax(dim=1).detach()\n","      # ...\n","\n","\n","    # вернуть прогнозы для каждого эл-та последовательности\n","    # batch x seq x n_en_token\n","    return ..."],"metadata":{"id":"tegOiKQKobjb"},"id":"tegOiKQKobjb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(embedding_dim=100, decoder_hidden_size=300)\n","decoder(encoder_output, en)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d51M66Oxof7J","executionInfo":{"status":"ok","timestamp":1700498914786,"user_tz":-180,"elapsed":2,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"0504423f-d9fa-43b6-8c4b-f175d9cae63f"},"id":"d51M66Oxof7J","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Ellipsis"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, emb_dim, hidden_size, n_ru):\n","        super().init()\n","        self.emb = nn.Embedding(num_embeddings=n_en_tokens, embedding_dim=emb_dim, padding_idx=0)\n","        nn.emb = GRUCell(embedding_dim, hidden_size) # gru mojno\n","        nn.fc1 = nn.Linear()\n","\n","    def forward(self, encoder_output, labels): # правильные ответы на языке на который переводим\n","        # labels: batch x seq_len\n","        # encoder_output: 1 x batch x encoder_hidden_size # считаем что в нулвом столбце - сос\n","        input_tokens = labels[:,0]\n","        decoder_hidden = encoder_output[0]\n","\n","        seq_len = labels.size(1)\n","\n","        for _ in range(1, seq_len):\n","            out = self.emb(input_tokens).relu() # batch_size x emb_dim\n","            decoder_hidden = self.rnn(out, decoder_hidden) # batch x decoder_hidden\n","            out = self.fc(decoder_hidden) # batch x n_en_tokens\n","\n","            input_tokens = out.argmax(dim=1).detach()\n","\n","        return # все сохраненные ауты"],"metadata":{"id":"z2ACUeaFusDB"},"id":"z2ACUeaFusDB","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N3nGjdJ_vTOc"},"id":"N3nGjdJ_vTOc","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"4d7b6d63","metadata":{"id":"4d7b6d63"},"source":["## Задачи для самостоятельного решения"]},{"cell_type":"markdown","id":"20525395","metadata":{"id":"20525395"},"source":["<p class=\"task\" id=\"1\"></p>\n","\n","1\\. Считайте файлы `RuBQ_2.0_train.json` (обучающее множество) и `RuBQ_2.0_test.json` (тестовое множество). Для каждого файла создайте по списка: список предложений на русском языке и список предложений на английском языке. Выведите на экран количество примеров в обучающей и тестовой выборке.\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","source":["import pandas as pd\n","\n","train_df = pd.read_json('RuBQ_2.0_train.json')\n","test_df = pd.read_json('RuBQ_2.0_test.json')"],"metadata":{"id":"mU2gdy9l4NGU","executionInfo":{"status":"ok","timestamp":1700596608759,"user_tz":-180,"elapsed":1394,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"mU2gdy9l4NGU","execution_count":2,"outputs":[]},{"cell_type":"code","source":["train_df.head(3)"],"metadata":{"id":"xUHeBPBt4NMz","colab":{"base_uri":"https://localhost:8080/","height":458},"executionInfo":{"status":"ok","timestamp":1700596608759,"user_tz":-180,"elapsed":16,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"9aca376d-3dac-4784-f266-d87e9650391b"},"id":"xUHeBPBt4NMz","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   uid                          question_text  \\\n","0    0              Что может вызвать цунами?   \n","1    1  Кто написал роман «Хижина дяди Тома»?   \n","2    2   Кто автор пьесы «Ромео и Джульетта»?   \n","\n","                                               query    answer_text  \\\n","0  SELECT ?answer \\nWHERE {\\n  wd:Q8070 wdt:P828 ...  Землетрясение   \n","1  SELECT ?answer \\nWHERE {\\n  wd:Q2222 wdt:P50 ?...  Г. Бичер-Стоу   \n","2  SELECT ?answer \\nWHERE {\\n  wd:Q83186 wdt:P50 ...        Шекспир   \n","\n","                             question_uris question_props  \\\n","0   [http://www.wikidata.org/entity/Q8070]     [wdt:P828]   \n","1   [http://www.wikidata.org/entity/Q2222]      [wdt:P50]   \n","2  [http://www.wikidata.org/entity/Q83186]      [wdt:P50]   \n","\n","                                             answers  \\\n","0  [{'type': 'uri', 'label': 'землетрясение', 'va...   \n","1  [{'type': 'uri', 'label': 'Гарриет Бичер-Стоу'...   \n","2  [{'type': 'uri', 'label': 'Уильям Шекспир', 'v...   \n","\n","                                     paragraphs_uids     tags  RuBQ_version  \\\n","0  {'with_answer': [35622], 'all_related': [35622...  [1-hop]             1   \n","1  {'with_answer': [35652], 'all_related': [35652...  [1-hop]             1   \n","2  {'with_answer': [35676, 35677], 'all_related':...  [1-hop]             1   \n","\n","                                        question_eng  \n","0                          What can cause a tsunami?  \n","1           Who wrote the novel \"uncle Tom's Cabin\"?  \n","2  Who is the author of the play \"Romeo and Juliet\"?  "],"text/html":["\n","  <div id=\"df-2f523922-4547-4a3d-b105-b9c526f577d8\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>uid</th>\n","      <th>question_text</th>\n","      <th>query</th>\n","      <th>answer_text</th>\n","      <th>question_uris</th>\n","      <th>question_props</th>\n","      <th>answers</th>\n","      <th>paragraphs_uids</th>\n","      <th>tags</th>\n","      <th>RuBQ_version</th>\n","      <th>question_eng</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Что может вызвать цунами?</td>\n","      <td>SELECT ?answer \\nWHERE {\\n  wd:Q8070 wdt:P828 ...</td>\n","      <td>Землетрясение</td>\n","      <td>[http://www.wikidata.org/entity/Q8070]</td>\n","      <td>[wdt:P828]</td>\n","      <td>[{'type': 'uri', 'label': 'землетрясение', 'va...</td>\n","      <td>{'with_answer': [35622], 'all_related': [35622...</td>\n","      <td>[1-hop]</td>\n","      <td>1</td>\n","      <td>What can cause a tsunami?</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Кто написал роман «Хижина дяди Тома»?</td>\n","      <td>SELECT ?answer \\nWHERE {\\n  wd:Q2222 wdt:P50 ?...</td>\n","      <td>Г. Бичер-Стоу</td>\n","      <td>[http://www.wikidata.org/entity/Q2222]</td>\n","      <td>[wdt:P50]</td>\n","      <td>[{'type': 'uri', 'label': 'Гарриет Бичер-Стоу'...</td>\n","      <td>{'with_answer': [35652], 'all_related': [35652...</td>\n","      <td>[1-hop]</td>\n","      <td>1</td>\n","      <td>Who wrote the novel \"uncle Tom's Cabin\"?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Кто автор пьесы «Ромео и Джульетта»?</td>\n","      <td>SELECT ?answer \\nWHERE {\\n  wd:Q83186 wdt:P50 ...</td>\n","      <td>Шекспир</td>\n","      <td>[http://www.wikidata.org/entity/Q83186]</td>\n","      <td>[wdt:P50]</td>\n","      <td>[{'type': 'uri', 'label': 'Уильям Шекспир', 'v...</td>\n","      <td>{'with_answer': [35676, 35677], 'all_related':...</td>\n","      <td>[1-hop]</td>\n","      <td>1</td>\n","      <td>Who is the author of the play \"Romeo and Juliet\"?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f523922-4547-4a3d-b105-b9c526f577d8')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2f523922-4547-4a3d-b105-b9c526f577d8 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2f523922-4547-4a3d-b105-b9c526f577d8');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-af718180-c68b-4c00-8729-4c91715b8f76\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af718180-c68b-4c00-8729-4c91715b8f76')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-af718180-c68b-4c00-8729-4c91715b8f76 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["len(train_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y9etOMBjEir6","executionInfo":{"status":"ok","timestamp":1700596608759,"user_tz":-180,"elapsed":13,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"adc65c2b-4ec4-4e9a-9582-1e5b66b68089"},"id":"y9etOMBjEir6","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2330"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["rus_sents = train_df['question_text'].values.tolist()\n","en_sents = train_df['question_eng'].values.tolist()\n","\n","rus_sents_test = test_df['question_text'].values.tolist()\n","en_sents_test = test_df['question_eng'].values.tolist()\n","len(rus_sents), len(rus_sents_test)"],"metadata":{"id":"1wKjhKHy4NTz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700596608760,"user_tz":-180,"elapsed":12,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"902d3a67-512f-46a3-c4d0-4260d6abc99b"},"id":"1wKjhKHy4NTz","execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2330, 580)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","id":"51d8edd3","metadata":{"id":"51d8edd3"},"source":["<p class=\"task\" id=\"2\"></p>\n","\n","2\\. Создайте два Vocab на основе загруженных данных: `ru_vocab` для слов на русском языке и `en_vocab` для слов на английском языке (словари создаются на основе обучающего множества). Добавьте в словари специальные токены `<PAD>`, `<SOS>`, `<EOS>`. Выведите на экран количество токенов в полученных словарях. Выведите на экран максимальное кол-во слов в предложениях на русском языке и в предложениях на английском языке (в обучающей выборке).\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gh6Wxfd2J25H","executionInfo":{"status":"ok","timestamp":1700596609577,"user_tz":-180,"elapsed":828,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"55502336-ba81-4aa8-dc49-6d10f1ac7303"},"id":"gh6Wxfd2J25H","execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import re\n","from nltk.tokenize import word_tokenize\n","\n","def get_corpus(sents):\n","    corpus = []\n","    for sent in sents:\n","      sent = re.sub(r'[^\\w\\s]+', '', sent.lower().strip())\n","      words = word_tokenize(sent)\n","      corpus.append(words)\n","    return corpus\n","\n","corpus_ru = get_corpus(rus_sents)\n","corpus_en = get_corpus(en_sents)\n","corpus_ru_test = get_corpus(rus_sents_test)\n","corpus_en_test = get_corpus(en_sents_test)"],"metadata":{"id":"mQRaqzXIE7O7","executionInfo":{"status":"ok","timestamp":1700596609578,"user_tz":-180,"elapsed":2,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"mQRaqzXIE7O7","execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","ru_vocab = build_vocab_from_iterator(corpus_ru, specials=['<pad>', '<unk>', '<sos>', '<eos>'])\n","ru_vocab.set_default_index(ru_vocab['<unk>'])\n","en_vocab = build_vocab_from_iterator(corpus_en, specials=['<pad>', '<unk>', '<sos>', '<eos>'])\n","en_vocab.set_default_index(en_vocab['<unk>'])"],"metadata":{"id":"Cq0rwqTJKmY5","executionInfo":{"status":"ok","timestamp":1700596611207,"user_tz":-180,"elapsed":1631,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"Cq0rwqTJKmY5","execution_count":8,"outputs":[]},{"cell_type":"code","source":["len(ru_vocab.get_stoi()), len(en_vocab.get_stoi())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iAR-01GMMHKO","executionInfo":{"status":"ok","timestamp":1700596611207,"user_tz":-180,"elapsed":15,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"96431301-5530-4eb3-adcf-da8d2b745feb"},"id":"iAR-01GMMHKO","execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5923, 4333)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["max([len(sent) for sent in corpus_ru]), max([len(sent) for sent in corpus_en])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZKESUYMIMQaQ","executionInfo":{"status":"ok","timestamp":1700596611207,"user_tz":-180,"elapsed":13,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"5afdc742-2778-409a-b282-ed909844fee7"},"id":"ZKESUYMIMQaQ","execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25, 31)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","id":"a1d666a1","metadata":{"id":"a1d666a1"},"source":["<p class=\"task\" id=\"3\"></p>\n","\n","3\\. Создайте класс `RuEnDataset`. Реализуйте `__getitem__` таким образом, чтобы он возвращал кортеж `(x, y)`, где x - это набор индексов токенов для предложений на русском языке, а `y` - набор индексов токенов для предложений на английском языке. Используя преобразования, сделайте длины наборов индексов одинаковой фиксированной длины, добавьте в начало каждого набора индекс `<SOS>`, а в конец - индекс токена `<EOS>`. Создайте датасет для обучающей и тестовой выборки.\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import torchtext.transforms as T\n","\n","class RuEnDataset(Dataset):\n","    def __init__(self, corpus_ru, corpus_en, ru_vocab, en_vocab):\n","        self.corpus_ru = corpus_ru\n","        self.corpus_en = corpus_en\n","        self.ru_vocab = ru_vocab\n","        self.en_vocab = en_vocab\n","        self.trans_ru = T.Sequential(\n","            T.AddToken('<sos>', begin=True),\n","            T.AddToken('<eos>', begin=False),\n","            T.VocabTransform(ru_vocab),\n","            T.ToTensor(0),\n","            T.PadTransform(max_length=self.get_max_len(), pad_value=0)\n","        )\n","        self.trans_en = T.Sequential(\n","            T.AddToken('<sos>', begin=True),\n","            T.AddToken('<eos>', begin=False),\n","            T.VocabTransform(en_vocab),\n","            T.ToTensor(0),\n","            T.PadTransform(max_length=self.get_max_len() + 2, pad_value=0)\n","        )\n","\n","    def __getitem__(self, idx):\n","        a = self.corpus_ru[idx]\n","        b = self.corpus_en[idx]\n","        return self.trans_ru(a), self.trans_en(b)\n","\n","    def __len__(self):\n","        return len(self.corpus_en)\n","\n","    def get_max_len(self):\n","        return max(len(y) for y in self.corpus_en)"],"metadata":{"id":"bdOgqVfTMi-C","executionInfo":{"status":"ok","timestamp":1700596611207,"user_tz":-180,"elapsed":11,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"bdOgqVfTMi-C","execution_count":11,"outputs":[]},{"cell_type":"code","source":["ru_en_dset = RuEnDataset(corpus_ru, corpus_en, ru_vocab, en_vocab)\n","ru_en_dset[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZZ-HS1YVhJa","executionInfo":{"status":"ok","timestamp":1700596611207,"user_tz":-180,"elapsed":10,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"9cfa8c84-4e55-418d-a760-c89b9d2aec20"},"id":"BZZ-HS1YVhJa","execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([   2,   27,  688, 2304, 5691,    3,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0]),\n"," tensor([   2,    6,  148,  236,   22, 4080,    3,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0]))"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["train_dset = RuEnDataset(corpus_ru, corpus_en, ru_vocab, en_vocab)\n","test_dset = RuEnDataset(corpus_ru_test, corpus_en_test, ru_vocab, en_vocab)"],"metadata":{"id":"vV0IQ1t5V7ep","executionInfo":{"status":"ok","timestamp":1700596611207,"user_tz":-180,"elapsed":8,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"vV0IQ1t5V7ep","execution_count":13,"outputs":[]},{"cell_type":"markdown","id":"ca51f9bd","metadata":{"id":"ca51f9bd"},"source":["<p class=\"task\" id=\"4\"></p>\n","\n","4\\. Опишите модель `Encoder`, которая возвращает скрытое состояние рекуррентного слоя в соотстветствии со следующей схемой. Пропустите через эту модель первые 16 предложений на русском языке и выведите размер полученного результата на экран. Результатом должен являться тензор размера `1 x batch_size x hidden_dim` (если используется один однонаправленный рекуррентный слой и `batch_first=True`).\n","\n","* количество эмбеддингов равно количеству слов на русском языке;\n","* размерность эмбеддингов выберите самостоятельно;\n","* при создании слоя эмбеддингов укажите `padding_idx`;\n","* размер скрытого состояния рекуррентного слоя выберите самостоятельно.\n","\n","![encoder](https://i0.wp.com/www.adeveloperdiary.com/wp-content/uploads/2020/10/Machine-Translation-using-Recurrent-Neural-Network-and-PyTorch-adeveloperdiary.com-1.png?w=815&ssl=1)\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","  def __init__(self, n_ru_tokens, embedding_dim, hidden_size):\n","      super().__init__()\n","      self.emb = nn.Embedding(\n","          num_embeddings=n_ru_tokens,\n","          embedding_dim=embedding_dim,\n","          padding_idx=0\n","      )\n","      self.dropout = nn.Dropout(p=0.5)\n","      self.rnn = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n","\n","  def forward(self, X):\n","    out = self.emb(X) # batch x seq x emb_size\n","    out = self.dropout(out)\n","    _, h = self.rnn(out) # out: batch x seq x hidden_size\n","    return h # 1 x batch x hidden_size"],"metadata":{"id":"VWDa9QZ65RPg","executionInfo":{"status":"ok","timestamp":1700596611207,"user_tz":-180,"elapsed":8,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"VWDa9QZ65RPg","execution_count":14,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","dl = DataLoader(train_dset, batch_size=16)\n","n_ru_tokens = len(ru_vocab.get_stoi())\n","embedding_dim = 350\n","hidden_size = 200\n","\n","encoder = Encoder(n_ru_tokens, embedding_dim, hidden_size)\n","for X, y in dl:\n","    h = encoder(X)\n","    break\n","h.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5WhzHOWC5ruO","executionInfo":{"status":"ok","timestamp":1700596611207,"user_tz":-180,"elapsed":8,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"2faeeb69-d25c-4023-840d-1b1e572e442c"},"id":"5WhzHOWC5ruO","execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 16, 200])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","id":"d94f101d","metadata":{"id":"d94f101d"},"source":["<p class=\"task\" id=\"5\"></p>\n","\n","5\\. Опишите модель `Decoder`, которая возвращает прогноз (набор индексов слов на английском языке). Пропустите через эту модель тензор скрытых состояний кодировщика, полученный в предыдущей задачи, и выведите размер полученного результата на экран. Результатом должен являться тензор размера `batch_size x seq_len x n_en_words` (если используется один однонаправленный рекуррентный слой и `batch_first=True`).\n","\n","* количество эмбеддингов равно количеству слов на английском языке;\n","* размер выходного слоя равен количеству слов на английском языке;\n","* размерность эмбеддингов выберите самостоятельно;\n","* при создании слоя эмбеддингов укажите `padding_idx`;\n","* размер скрытого состояния рекуррентного слоя выберите самостоятельно.\n","\n","![decoder](https://i2.wp.com/www.adeveloperdiary.com/wp-content/uploads/2020/10/Machine-Translation-using-Recurrent-Neural-Network-and-PyTorch-adeveloperdiary.com-2.png?w=899&ssl=1)\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, n_en_tokens, embedding_dim, decoder_hidden_size):\n","        super().__init__()\n","        self.n_en_tokens = n_en_tokens\n","        self.emb = nn.Embedding(\n","              num_embeddings=n_en_tokens,\n","              embedding_dim=embedding_dim,\n","              padding_idx=0\n","        )\n","        self.rnn = nn.GRU(embedding_dim, decoder_hidden_size, batch_first=True)\n","        self.fc = nn.Linear(decoder_hidden_size, n_en_tokens)\n","\n","    def forward(self, encoder_output, labels): # правильные ответы на языке на который переводим\n","        # labels: batch_size x seq_len\n","        # encoder_output: 1 x batch x encoder_hidden_size # считаем что в нулвом столбце - сос\n","        input_tokens = labels[:,0]\n","        decoder_hidden = encoder_output[0]\n","\n","        target_len = labels.size(0)\n","        seq_len = labels.size(1)\n","        target_vocab_size = self.n_en_tokens\n","        #batch_size =\n","        predictions = th.zeros(target_len, seq_len, target_vocab_size)\n","        for i in range(1, seq_len):\n","            out = self.emb(input_tokens).relu() # batch_size x emb_dim\n","            decoder_hidden, h = self.rnn(out) # batch x decoder_hidden\n","            out = self.fc(decoder_hidden) # batch x n_en_tokens\n","\n","            #input_tokens = out.argmax(dim=1).detach()\n","            predictions[:,i - 1] = out\n","            input_tokens = labels[:, i]\n","\n","        return predictions # все сохраненные ауты"],"metadata":{"id":"yijMxtm772VA","executionInfo":{"status":"ok","timestamp":1700596611208,"user_tz":-180,"elapsed":7,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"yijMxtm772VA","execution_count":16,"outputs":[]},{"cell_type":"code","source":["embedding_dim = 350\n","decoder_hidden_size = 200\n","n_en_tokens = len(en_vocab.get_stoi())\n","\n","decoder = Decoder(n_en_tokens, embedding_dim, decoder_hidden_size)\n","\n","for X, y in dl:\n","    pred = decoder(h, y)\n","    break\n","pred.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0K9p-613GF2J","executionInfo":{"status":"ok","timestamp":1700596611208,"user_tz":-180,"elapsed":7,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"804568b2-c8bb-466e-9bba-f4ef5357dcd3"},"id":"0K9p-613GF2J","execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([16, 33, 4333])"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","id":"da2f75c8","metadata":{"id":"da2f75c8"},"source":["<p class=\"task\" id=\"6\"></p>\n","\n","6\\. Объедините модели `Encoder` и `Decoder` в одну модель `EncoderDecoder`. Пропустите через эту модель первые 16 предложений на русском языке и выведите размер полученного результата на экран. Сделайте полученный результат двумерным, объединив две первые размерности: `batch_size * seq_len x n_en_words`. Выведите размерность полученного результата на экран.\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","source":["class EncoderDecoder(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, X, y):\n","\n","        hidden = self.encoder(X)\n","        outputs = self.decoder(hidden, y)\n","\n","        return outputs"],"metadata":{"id":"Ph5FOw4bRnGq","executionInfo":{"status":"ok","timestamp":1700596611208,"user_tz":-180,"elapsed":6,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"Ph5FOw4bRnGq","execution_count":18,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(n_en_tokens, embedding_dim, decoder_hidden_size)\n","encoder_decoder = EncoderDecoder(encoder, decoder)\n","all = []\n","\n","for X_, y_ in dl:\n","    res = encoder_decoder.forward(X_, y_)\n","    break\n","res.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UKyiBU_BTf1x","executionInfo":{"status":"ok","timestamp":1700596620390,"user_tz":-180,"elapsed":5,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"34c42662-8007-4bc3-a425-b07aac12d68c"},"id":"UKyiBU_BTf1x","execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([16, 33, 4333])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["res.reshape(-1, res.size(2)).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7YKO3FT2UZoF","executionInfo":{"status":"ok","timestamp":1700596623365,"user_tz":-180,"elapsed":3,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"3dad480f-729c-423c-a0c1-6a3abbf423e9"},"id":"7YKO3FT2UZoF","execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([528, 4333])"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","id":"ec784777","metadata":{"id":"ec784777"},"source":["<p class=\"task\" id=\"7\"></p>\n","\n","7\\. Настройте модель, решив задачу классификации на основе прогнозов модели `EncoderDecoder`. Игнорируйте токен `<PAD>` при расчете ошибки. Во время обучения выводите на экран значения функции потерь для эпохи (на обучающем множестве), значение accuracy по токенам (на обучающем множестве) и пример перевода, сгенерированного моделью. После завершения обучения посчитайте BLEU для тестового множества.\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"auYVrX_Hbx88","executionInfo":{"status":"ok","timestamp":1700596632661,"user_tz":-180,"elapsed":7379,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"1175a037-03f6-4290-b4bf-07460f5e4a74"},"id":"auYVrX_Hbx88","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.2.0)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.10.0)\n","Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"]}]},{"cell_type":"code","source":["import torch.optim as optim\n","import torchmetrics"],"metadata":{"id":"_WdRhHrZZEQE","executionInfo":{"status":"ok","timestamp":1700596635029,"user_tz":-180,"elapsed":2370,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"_WdRhHrZZEQE","execution_count":23,"outputs":[]},{"cell_type":"code","source":["n_epochs = 7\n","lr = 0.01\n","device = 'cuda' if th.cuda.is_available() else 'cpu'\n","decoder = Decoder(n_en_tokens, embedding_dim, decoder_hidden_size)\n","model = EncoderDecoder(encoder, decoder).to(device=device)\n","crit = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","all = []\n","\n","\n","for epoch in range(n_epochs):\n","    losses = []\n","    acc_m = torchmetrics.Accuracy(task='multiclass', num_classes=len(en_vocab.get_stoi()), ignore_index=0)\n","    for X, y in dl:\n","        out = model.forward(X.to(device=device), y.to(device=device))\n","        all.append(out.argmax(dim=2))\n","        loss = crit(out.reshape(-1, out.size(2)), y.view(-1))\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        losses.append(loss)\n","        acc_m.update(\n","            out.reshape(-1, out.size(2)).argmax(dim=1).to(device='cpu'),\n","            y.view(-1)\n","        )\n","\n","    print('epoch= ', epoch)\n","    print('loss= ', th.tensor(losses).mean().item())\n","    check = out.argmax(dim=2)[0].reshape(-1).tolist()\n","    ans = en_vocab.lookup_tokens(check)\n","    print(' '.join(ans))\n","    print('accuracy= ', acc_m.compute().item())\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZeyIdcGGWxVo","executionInfo":{"status":"ok","timestamp":1700596772845,"user_tz":-180,"elapsed":132240,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"89604179-8a4f-4a74-c23c-7eb4cb20b504"},"id":"ZeyIdcGGWxVo","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch=  0\n","loss=  2.8213284015655518\n","<sos> how tall was live freezing <eos> c c c c c c c c c c c c c c c c c c c c c c c c c <pad>\n","accuracy=  0.6868424415588379\n","\n","epoch=  1\n","loss=  1.4861609935760498\n","<sos> how tall was participate mars <eos> c c c c c c c c c c c c c c c c c c c c c c c c c <pad>\n","accuracy=  0.7993064522743225\n","\n","epoch=  2\n","loss=  1.0277494192123413\n","<sos> how tall was objects c <eos> c c c c c c c c c c c c c c c c c c c c c c c c c <pad>\n","accuracy=  0.8367025256156921\n","\n","epoch=  3\n","loss=  0.6518498659133911\n","<sos> how tall was bush senior <eos> sources sources sources sources sources sources sources sources sources sources sources sources sources sources sources sources sources sources sources sources sources sources sources sources sources <pad>\n","accuracy=  0.8843047022819519\n","\n","epoch=  4\n","loss=  0.30423668026924133\n","<sos> how tall was bush senior <eos> equator equator equator equator equator equator equator equator equator equator equator equator equator equator equator equator equator equator equator equator equator equator equator equator equator <pad>\n","accuracy=  0.9375813007354736\n","\n","epoch=  5\n","loss=  0.0961536392569542\n","<sos> how tall was bush senior <eos> toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma <pad>\n","accuracy=  0.9839224219322205\n","\n","epoch=  6\n","loss=  0.01132952980697155\n","<sos> how tall was bush senior <eos> toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma toma <pad>\n","accuracy=  0.9992907047271729\n","\n"]}]},{"cell_type":"code","source":["dl_test = DataLoader(test_dset, batch_size=16)"],"metadata":{"id":"bhE4F4Wsb6MU","executionInfo":{"status":"ok","timestamp":1700596922606,"user_tz":-180,"elapsed":6,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"bhE4F4Wsb6MU","execution_count":32,"outputs":[]},{"cell_type":"code","source":["translated, tr_y = [], []\n","for X, y in dl_test:\n","    out = model.forward(X.to(device=device), y.to(device=device))\n","    out = out.argmax(dim=2)\n","    for sent in out:\n","        translated.append(en_vocab.lookup_tokens(sent.tolist()))\n","    for Y in y:\n","        tr_y.append(en_vocab.lookup_tokens(Y.tolist()))"],"metadata":{"id":"K3yOctb5-lfs","executionInfo":{"status":"ok","timestamp":1700597515069,"user_tz":-180,"elapsed":1505,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"K3yOctb5-lfs","execution_count":49,"outputs":[]},{"cell_type":"code","source":["trs, trs_y = [], []\n","for lst in translated:\n","    if \"<eos>\" in lst:\n","        index = lst.index(\"<eos>\")\n","    trs.append(lst[1:index])\n","\n","for lst in tr_y:\n","    if \"<eos>\" in lst:\n","        index = lst.index(\"<eos>\")\n","    trs_y.append(lst[1:index])"],"metadata":{"id":"Jl3FJyWKhAnV","executionInfo":{"status":"ok","timestamp":1700598391895,"user_tz":-180,"elapsed":7,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"Jl3FJyWKhAnV","execution_count":80,"outputs":[]},{"cell_type":"code","source":["ref = []\n","for sent in corpus_en_test:\n","    ref.append([sent])"],"metadata":{"id":"10JGBkgNkFvv","executionInfo":{"status":"ok","timestamp":1700598599488,"user_tz":-180,"elapsed":6,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}}},"id":"10JGBkgNkFvv","execution_count":86,"outputs":[]},{"cell_type":"code","source":["from torchtext.data.metrics import bleu_score\n","\n","candidate_corpus = trs\n","reference_corpus = ref\n","bleu_score(candidate_corpus, reference_corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SsVP79JJ1q38","executionInfo":{"status":"ok","timestamp":1700598610159,"user_tz":-180,"elapsed":7,"user":{"displayName":"Софья Косицкая","userId":"11161806994107976584"}},"outputId":"be5ba57c-a839-4abe-d291-ef2b20c0eb35"},"id":"SsVP79JJ1q38","execution_count":87,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6965598294299764"]},"metadata":{},"execution_count":87}]},{"cell_type":"markdown","id":"66caa919","metadata":{"id":"66caa919"},"source":["## Обратная связь\n","- [x] Хочу получить обратную связь по решению"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}